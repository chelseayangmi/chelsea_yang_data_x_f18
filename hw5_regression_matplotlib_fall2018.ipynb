{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X Spring 2018: Homework 05\n",
    "\n",
    "### Linear regression, logistic regression, matplotlib.\n",
    "\n",
    "\n",
    "In this homework, you will do some exercises with prediction and plotting. \n",
    "\n",
    "REMEMBER TO DISLPAY ALL OUTPUTS. If the question asks you to do something, make sure to print your results so we can easily see that you have done it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data:\n",
    "__Data Source__:\n",
    "Data file is uploaded to bCourses and is named: __Energy.csv__\n",
    "\n",
    "The dataset was created by Angeliki Xifara ( Civil/Structural Engineer) and was processed by Athanasios Tsanas, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).\n",
    "\n",
    "__Data Description__:\n",
    "\n",
    "The dataset contains eight attributes of a building (or features, denoted by X1...X8) and response being the heating load on the building, y1. \n",
    "\n",
    "* X1\tRelative Compactness \n",
    "* X2\tSurface Area \n",
    "* X3\tWall Area \n",
    "*  X4\tRoof Area \n",
    "*  X5\tOverall Height \n",
    "* X6\tOrientation \n",
    "*  X7\tGlazing Area \n",
    "*  X8\tGlazing Area Distribution \n",
    "*  y1\tHeating Load \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1\n",
    "Read the data file in python. Check if there are any NaN values, and print the results.\n",
    "\n",
    "Describe data features in terms of type, distribution range (max and min), and mean values. \n",
    "\n",
    "Plot feature distributions. This step should give you clues about data sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __REGRESSION__:\n",
    "LABELS ARE CONTINUOUS VALUES.\n",
    "Here the model is trained to predict a continuous value for each instance.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a continuous value  for  that instance.  \n",
    "\n",
    "__Q 1.2: Train a linear regression model on 80 percent of the given dataset, what is the intercept value and coefficient values.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Q.1.3: Report model performance using 'ROOT MEAN SQUARE' error metric on:  \n",
    "__1. Data that was used for training(Training error)__   \n",
    "__2. On the 20 percent of unseen data (test error) __ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q1.4: \n",
    "\n",
    "Lets us see the effect of amount of data on the performance of prediction model. Use varying amounts of  Training data (100,200,300,400,500,all) to train regression models and report  training error and validation error in each case. Validation data/Test data  is the same as above for  all  these cases.\n",
    "\n",
    "**Plot error rates vs number of training examples.** Both the training error and the validation error should be plotted. Comment on the relationship you observe in the plot, between the amount of data used to train the model and the validation accuracy of the model.\n",
    "\n",
    "__Hint:__ Use array indexing to choose varying data amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "Here the model is trained to classify each instance into a set of predefined  discrete classes.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a  class of that instance. You can also output the probabilities of an instance belnging to a class.  \n",
    "\n",
    "__ Q 2.1:  Bucket values of 'y1' i.e 'Heating Load'  from the original dataset into 3 classes:__ \n",
    "\n",
    "0: 'Low' ( < 14),   \n",
    "1: 'Medium'  (14-28),   \n",
    "2: 'High'  (>28)\n",
    "\n",
    "This converts the given dataset  into a classification problem, classes being, Heating load is: *low, medium or high*. Use this datset with transformed 'heating load' for creating a  logistic regression classifiction model that predicts heating load type of a building. Use test-train split ratio of 0.8 : 0.2.  \n",
    "\n",
    "*Report training and test accuracies and  confusion matrices.*\n",
    "\n",
    "\n",
    "**HINT:** Use pandas.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Q2.2: \n",
    "One of the preprocessing steps in Data science is Feature Scaling i.e getting all our data on the same scale by setting same  Min-Max of feature values. \n",
    "This makes training less sensitive to the scale of features . \n",
    "Scaling is important in algorithms that use distance based classification, SVM or K means or those that involve gradient descent optimization. If we  Scale features in the range [0,1] it is called unity based normalization.__\n",
    "\n",
    "__Perform unity based normalization on the above dataset and train the model again, compare model performance in training and validation with your previous model.__  \n",
    "\n",
    "refer:http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler  \n",
    "more at: https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3 - Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3.1a. Create a dataframe called `icecream` that has column `Flavor` with entries `Strawberry`, `Vanilla`, and `Chocolate` and another column with `Price` with entries `3.50`, `3.00`, and `4.25`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3.1b \n",
    "Create a bar chart representing the three flavors and their associated prices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 3.2   Create 9 random plots (Hint: There is a numpy function for generating random data). The top three should be scatter plots (one with green dots, one with purple crosses, and one with blue triangles. The middle three graphs should be a line graph, a horizontal bar chart, and a histogram. The bottom three graphs should be trignometric functions (one sin, one cosine, one tangent).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "data-x"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
